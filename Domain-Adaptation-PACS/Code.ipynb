{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework3",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9QcGnGPdX2C"
      },
      "source": [
        "\n",
        "**Install requirements**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9O3aM3Tb28q"
      },
      "source": [
        "#!pip3 install 'torch==1.3.1'\n",
        "#!pip3 install 'torchvision==0.5.0'\n",
        "#!pip3 install 'Pillow-SIMD'\n",
        "#!pip3 install 'tqdm'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo942LMOdlh4"
      },
      "source": [
        "**Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DokFOdD1dJEl"
      },
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models import alexnet\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIDLJuIXK_vh"
      },
      "source": [
        "**Set Arguments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5PkYfqfK_SA"
      },
      "source": [
        "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
        "\n",
        "NUM_CLASSES = 7 # 101 + 1: There is am extra Background class that should be removed \n",
        "\n",
        "BATCH_SIZE = 64    # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
        "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
        "\n",
        "LR = 5e-3            # The initial Learning Rate\n",
        "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default\n",
        "alpha=0.05\n",
        "NUM_EPOCHS = 25      # Total number of training epochs (iterations over dataset)\n",
        "STEP_SIZE = 15      # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n",
        "\n",
        "LOG_FREQUENCY = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ubzr_UHz8KqF",
        "outputId": "fed145f0-e877-452b-cc0c-23eaa0d49dc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "\n",
        "acc_1 = [30.87, 19.16, 26.64]\n",
        "acc_5 = [21.32, 27.00, 29.37 ]\n",
        "acc_10 = [22.37, 19.13, 26.31]\n",
        "test_lr = [\"5e-4\", \"1e-3\", \"5e-3\"]\n",
        "\n",
        "\n",
        "plt.plot(test_lr, acc_1, 'g', label='With 0.01 alpha')\n",
        "plt.plot(test_lr, acc_5, 'b', label='With 0.05 alpha')\n",
        "plt.plot(test_lr, acc_10, 'r', label='With 0.1 alpha')\n",
        "plt.title('Accuracy in Sketch Data')\n",
        "plt.xlabel('Learning rate')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gwii0TBHvzh"
      },
      "source": [
        "**Define Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUDdw4j2H0Mc"
      },
      "source": [
        "# Define transforms for training phase\n",
        "train_transform = transforms.Compose([transforms.Resize(256),      # Resizes short size of the PIL image to 256\n",
        "                                      transforms.CenterCrop(224),  # Crops a central square patch of the image\n",
        "                                                                   # 224 because torchvision's AlexNet needs a 224x224 input!\n",
        "                                                                   # Remember this when applying different transformations, otherwise you get an error\n",
        "                                      #transforms.RandomCrop( 64 , padding =2) ,\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) # Normalizes tensor with mean and standard deviation\n",
        "])  \n",
        "# Define transforms for the evaluation phase\n",
        "eval_transform = transforms.Compose([transforms.Resize(256),\n",
        "                                      transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))                                   \n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qYIHPzYLY7i"
      },
      "source": [
        "**Prepare Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfVq_uDHLbsR",
        "outputId": "35889e5e-cc3f-49f7-9cd6-3f76e265ced0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# Clone github repository with data\n",
        "if not os.path.isdir('./Homework3-PACS'):\n",
        "  !git clone https://github.com/MachineLearning2020/Homework3-PACS.git\n",
        "  \n",
        "\n",
        "DATA_DIR_TRAIN = 'Homework3-PACS/PACS/photo'\n",
        "DATA_DIR_TEST = 'Homework3-PACS/PACS/art_painting'\n",
        "DATA_DIR_VAL = 'Homework3-PACS/PACS/cartoon'\n",
        "\n",
        "\n",
        "# Prepare Pytorch train/test Datasets\n",
        "train_dataset = torchvision.datasets.ImageFolder(DATA_DIR_TRAIN, transform=train_transform)\n",
        "test_dataset = torchvision.datasets.ImageFolder(DATA_DIR_TEST, transform=eval_transform)\n",
        "val_dataset = torchvision.datasets.ImageFolder(DATA_DIR_VAL, transform=eval_transform)\n",
        "\n",
        "'''\n",
        "train_indexes = [idx for idx in range(len(train_dataset)) if idx % 5]\n",
        "test_indexes = [idx for idx in range(len(test_dataset)) if not idx % 5]\n",
        "\n",
        "train_dataset = Subset(train_dataset, train_indexes)\n",
        "test_dataset = Subset(test_dataset, test_indexes)\n",
        "'''\n",
        "\n",
        "# Check dataset sizes\n",
        "print('Train Dataset: {}'.format(len(train_dataset)))\n",
        "print('Test Dataset: {}'.format(len(test_dataset)))\n",
        "print('Validation Dataset: {}'.format(len(val_dataset)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Dataset: 1670\n",
            "Test Dataset: 2048\n",
            "Validation Dataset: 2344\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfOeh6VBkk7d"
      },
      "source": [
        "\n",
        "  \n",
        "\n",
        "from torch.autograd import Function\n",
        "\n",
        "\n",
        "\n",
        "class ReverseLayerF(Function):\n",
        "    # Forwards identity\n",
        "    # Sends backward reversed gradients\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, alpha):\n",
        "        ctx.alpha = alpha\n",
        "\n",
        "        return x.view_as(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        output = grad_output.neg() * ctx.alpha\n",
        "\n",
        "        return output, None\n",
        "\n",
        "        \n",
        "\n",
        "class AlexModel(nn.Module):\n",
        "   \n",
        "\n",
        "    def __init__(self):\n",
        "        super(AlexModel, self).__init__()\n",
        "        #self.restored = False\n",
        "        model_alexnet = alexnet(pretrained=True)\n",
        "        mod_alexnet = alexnet(pretrained=True)\n",
        "\n",
        "\n",
        "        self.features = model_alexnet.features\n",
        "\n",
        "         \n",
        "      \n",
        "        self.classifier=model_alexnet.classifier\n",
        "        self.discriminator=mod_alexnet.classifier\n",
        "       \n",
        "        \n",
        "    def forward(self, input_data, alpha=None):\n",
        "        input_data = input_data.expand(input_data.data.shape[0], 3, 224, 224)\n",
        "        features = self.features(input_data)\n",
        "        #features = features.view(features.size(0), -1)\n",
        "\n",
        "        features = features.view(-1, 256*6*6)\n",
        "        if alpha is not None:\n",
        "            # gradient reversal layer (backward gradients will be reversed)\n",
        "            reverse_feature = ReverseLayerF.apply(features, alpha)\n",
        "            discriminator_output = self.discriminator(reverse_feature)\n",
        "            return discriminator_output\n",
        "        # If we don't pass alpha, we assume we are training with supervision\n",
        "        else:\n",
        "            # do something else\n",
        "            class_outputs = self.classifier(features)\n",
        "            return class_outputs\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qr008cCysRi8",
        "outputId": "3616b300-cbc8-4a94-9f54-c84e13c10ad3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "net = AlexModel() # Loading AlexNet model\n",
        "net2= alexnet(pretrained=True)\n",
        "\n",
        "net.classifier[6]=nn.Linear(4096, NUM_CLASSES)\n",
        "net.discriminator[6]=nn.Linear(4096, 2)\n",
        "print(net.classifier[6])\n",
        "print(net.discriminator[6])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear(in_features=4096, out_features=7, bias=True)\n",
            "Linear(in_features=4096, out_features=2, bias=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYEDQ7Z21ldN"
      },
      "source": [
        "**Prepare Dataloaders**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VriRw8SI1nle"
      },
      "source": [
        "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbZ1t5Qs2z4j"
      },
      "source": [
        "**Prepare Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exHUjtXa22DN"
      },
      "source": [
        "\n",
        "#net2.classifier[6] = nn.Linear(4096, NUM_CLASSES)\n",
        "\n",
        "\n",
        "#print(net2.parameters)\n",
        "# AlexNet has 1000 output neurons, corresponding to the 1000 ImageNet's classes\n",
        "# We need 101 outputs for Caltech-101\n",
        "#net.classifier[6] = nn.Linear(4096, NUM_CLASSES) # nn.Linear in pytorch is a fully connected layer\n",
        "                                                 # The convolutional layer is nn.Conv2d\n",
        "\n",
        "# We just changed the last layer of AlexNet with a new fully connected layer with 101 outputs\n",
        "# It is mandatory to study torchvision.models.alexnet source code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEyL3H_R4qCf"
      },
      "source": [
        "**Prepare Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sjq00G94tSc"
      },
      "source": [
        "# Define loss function\n",
        "criterion = nn.CrossEntropyLoss() # for classification, we use Cross Entropy\n",
        "\n",
        "# Choose parameters to optimize\n",
        "# To access a different set of parameters, you have to access submodules of AlexNet\n",
        "# (nn.Module objects, like AlexNet, implement the Composite Pattern)\n",
        "# e.g.: parameters of the fully connected layers: net.classifier.parameters()\n",
        "# e.g.: parameters of the convolutional layers: look at alexnet's source code ;) \n",
        "parameters_to_optimize = net.parameters() \n",
        "\n",
        "\n",
        "  # In this case we optimize over all the parameters of AlexNet\n",
        "\n",
        "# Define optimizer\n",
        "# An optimizer updates the weights based on loss\n",
        "# We use SGD with momentum\n",
        "optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "#optimizer = optim.Adam(parameters_to_optimize, LR)\n",
        "\n",
        "# Define scheduler\n",
        "# A scheduler dynamically changes learning rate\n",
        "# The most common schedule is the step(-down), which multiplies learning rate by gamma every STEP_SIZE epochs\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxYUli9d9uYQ"
      },
      "source": [
        "**Train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcoQ5fD49yT_",
        "outputId": "d2476192-03d7-47e1-d31a-1818fefbcc72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#lr=0.0001\n",
        "# By default, everything is loaded to cpu\n",
        "net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "min_loss=0.\n",
        "min_epoch=0\n",
        "cudnn.benchmark # Calling this optimizes runtime\n",
        "\n",
        "current_step = 0\n",
        "# Start iterating over the epochs\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr()))\n",
        "  len_dataloader = min(len(train_dataloader), len(test_dataloader))\n",
        "  # Iterate over the dataset\n",
        "  data_zip = enumerate(zip(train_dataloader, test_dataloader))\n",
        "  #for images, labels in train_dataloader:\n",
        "  for step, ((images_source, labels_source), (images_target, _)) in data_zip:\n",
        "  #for images_source, labels_source in train_dataloader:  \n",
        "\n",
        "    size_source = len(images_source)\n",
        "    size_target = len(images_target)\n",
        "\n",
        "    # Bring data over the device of choice\n",
        "    images_source = images_source.to(DEVICE)\n",
        "    labels_source = labels_source.to(DEVICE)\n",
        "    images_target = images_target.to(DEVICE)\n",
        "\n",
        "    net.train() # Sets module in training mode\n",
        "\n",
        "   # p = float(step + epoch * len_dataloader) / NUM_EPOCHS / len_dataloader\n",
        "   # alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
        "   # lr = 0.01 / (1. + 10 * p)**0.75\n",
        "    #for param_group in optimizer.param_groups:\n",
        "        #param_group['lr'] = lr\n",
        "\n",
        "    # PyTorch, by default, accumulates gradients after each backward pass\n",
        "    # We need to manually set the gradients to zero before starting a new iteration\n",
        "    optimizer.zero_grad() # Zero-ing the gradients\n",
        "\n",
        "    # Forward pass to the network\n",
        "    output_source = net(images_source)\n",
        "    disc_source = torch.zeros(size_source).long().to(DEVICE)  # source 0\n",
        "    disc_target = torch.ones(size_target).long().to(DEVICE)  # target 1\n",
        "    # Compute loss based on output and ground truth\n",
        "    loss_source = criterion(output_source, labels_source)\n",
        "    loss_source.backward()\n",
        "\n",
        "    output_source_d = net(images_source, alpha)\n",
        "    loss_source_d = criterion(output_source_d, disc_source)\n",
        "    loss_source_d.backward()\n",
        "\n",
        "    output_target_d = net(images_target, alpha)\n",
        "    loss_target_d = criterion(output_target_d, disc_target)\n",
        "    loss_target_d.backward()\n",
        "\n",
        "    total_loss= loss_source+loss_source_d+loss_target_d\n",
        "   \n",
        "    if min_loss==0 or total_loss<min_loss:\n",
        "      min_loss=total_loss\n",
        "      min_epoch=epoch\n",
        "    \n",
        "\n",
        "    # Log loss\n",
        "    if current_step % LOG_FREQUENCY == 0:\n",
        "      print('Step {}, Loss source {} \\n'.format(current_step, loss_source.item()))\n",
        "      print('Step {}, Loss source discriminator {} \\n'.format(current_step, loss_source_d.item()))\n",
        "      print('Step {}, Loss target discriminator {} \\n'.format(current_step, loss_target_d.item()))\n",
        "      \n",
        "    # Compute gradients for each layer and update weights\n",
        "      # backward pass: computes gradients\n",
        "    optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "    current_step += 1\n",
        "    #print('Alpha factor {} p factor {} '.format(alpha, p))\n",
        "\n",
        "  # Step the scheduler\n",
        "  print('Min_loss {}, min_epoch {} \\n'.format(min_loss, min_epoch))\n",
        "  scheduler.step() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting epoch 1/25, LR = [0.005]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:351: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 0, Loss source 1.995077133178711 \n",
            "\n",
            "Step 0, Loss source discriminator 0.6738304495811462 \n",
            "\n",
            "Step 0, Loss target discriminator 0.8489561676979065 \n",
            "\n",
            "Step 10, Loss source 0.29830455780029297 \n",
            "\n",
            "Step 10, Loss source discriminator 0.10490924119949341 \n",
            "\n",
            "Step 10, Loss target discriminator 1.0352799892425537 \n",
            "\n",
            "Step 20, Loss source 0.6693989038467407 \n",
            "\n",
            "Step 20, Loss source discriminator 1.4416449069976807 \n",
            "\n",
            "Step 20, Loss target discriminator 1.420478343963623 \n",
            "\n",
            "Min_loss 0.534128725528717, min_epoch 0 \n",
            "\n",
            "Starting epoch 2/25, LR = [0.005]\n",
            "Step 30, Loss source 0.31866034865379333 \n",
            "\n",
            "Step 30, Loss source discriminator 0.2501995861530304 \n",
            "\n",
            "Step 30, Loss target discriminator 0.5830632448196411 \n",
            "\n",
            "Step 40, Loss source 0.35299986600875854 \n",
            "\n",
            "Step 40, Loss source discriminator 0.2792470455169678 \n",
            "\n",
            "Step 40, Loss target discriminator 0.35090765357017517 \n",
            "\n",
            "Step 50, Loss source 0.1091742217540741 \n",
            "\n",
            "Step 50, Loss source discriminator 0.2111159861087799 \n",
            "\n",
            "Step 50, Loss target discriminator 0.5530605912208557 \n",
            "\n",
            "Min_loss 0.25930678844451904, min_epoch 1 \n",
            "\n",
            "Starting epoch 3/25, LR = [0.005]\n",
            "Step 60, Loss source 0.042749520391225815 \n",
            "\n",
            "Step 60, Loss source discriminator 0.38719442486763 \n",
            "\n",
            "Step 60, Loss target discriminator 0.029344797134399414 \n",
            "\n",
            "Step 70, Loss source 0.19232690334320068 \n",
            "\n",
            "Step 70, Loss source discriminator 0.40324169397354126 \n",
            "\n",
            "Step 70, Loss target discriminator 0.10109306871891022 \n",
            "\n",
            "Min_loss 0.25930678844451904, min_epoch 1 \n",
            "\n",
            "Starting epoch 4/25, LR = [0.005]\n",
            "Step 80, Loss source 0.14210042357444763 \n",
            "\n",
            "Step 80, Loss source discriminator 0.46434450149536133 \n",
            "\n",
            "Step 80, Loss target discriminator 0.12177470326423645 \n",
            "\n",
            "Step 90, Loss source 0.06558385491371155 \n",
            "\n",
            "Step 90, Loss source discriminator 0.2008165419101715 \n",
            "\n",
            "Step 90, Loss target discriminator 0.1632593274116516 \n",
            "\n",
            "Step 100, Loss source 0.025685742497444153 \n",
            "\n",
            "Step 100, Loss source discriminator 0.20063702762126923 \n",
            "\n",
            "Step 100, Loss target discriminator 0.8469582796096802 \n",
            "\n",
            "Min_loss 0.20617887377738953, min_epoch 3 \n",
            "\n",
            "Starting epoch 5/25, LR = [0.005]\n",
            "Step 110, Loss source 0.21786020696163177 \n",
            "\n",
            "Step 110, Loss source discriminator 0.06006675958633423 \n",
            "\n",
            "Step 110, Loss target discriminator 0.44128501415252686 \n",
            "\n",
            "Step 120, Loss source 0.1516915261745453 \n",
            "\n",
            "Step 120, Loss source discriminator 0.06522727012634277 \n",
            "\n",
            "Step 120, Loss target discriminator 0.34557849168777466 \n",
            "\n",
            "Min_loss 0.20617887377738953, min_epoch 3 \n",
            "\n",
            "Starting epoch 6/25, LR = [0.005]\n",
            "Step 130, Loss source 0.018315128982067108 \n",
            "\n",
            "Step 130, Loss source discriminator 0.15796467661857605 \n",
            "\n",
            "Step 130, Loss target discriminator 0.10570520162582397 \n",
            "\n",
            "Step 140, Loss source 0.03519181162118912 \n",
            "\n",
            "Step 140, Loss source discriminator 0.6730088591575623 \n",
            "\n",
            "Step 140, Loss target discriminator 0.007247778587043285 \n",
            "\n",
            "Step 150, Loss source 0.12006211280822754 \n",
            "\n",
            "Step 150, Loss source discriminator 0.2563045024871826 \n",
            "\n",
            "Step 150, Loss target discriminator 0.17015254497528076 \n",
            "\n",
            "Min_loss 0.20617887377738953, min_epoch 3 \n",
            "\n",
            "Starting epoch 7/25, LR = [0.005]\n",
            "Step 160, Loss source 0.03274626284837723 \n",
            "\n",
            "Step 160, Loss source discriminator 0.22665801644325256 \n",
            "\n",
            "Step 160, Loss target discriminator 0.14181530475616455 \n",
            "\n",
            "Step 170, Loss source 0.1267174482345581 \n",
            "\n",
            "Step 170, Loss source discriminator 0.2721470594406128 \n",
            "\n",
            "Step 170, Loss target discriminator 0.034145090728998184 \n",
            "\n",
            "Step 180, Loss source 0.05089298263192177 \n",
            "\n",
            "Step 180, Loss source discriminator 0.09298577904701233 \n",
            "\n",
            "Step 180, Loss target discriminator 0.4234926402568817 \n",
            "\n",
            "Min_loss 0.15432412922382355, min_epoch 6 \n",
            "\n",
            "Starting epoch 8/25, LR = [0.005]\n",
            "Step 190, Loss source 0.01111745834350586 \n",
            "\n",
            "Step 190, Loss source discriminator 0.2876031696796417 \n",
            "\n",
            "Step 190, Loss target discriminator 0.039368461817502975 \n",
            "\n",
            "Step 200, Loss source 0.03213648498058319 \n",
            "\n",
            "Step 200, Loss source discriminator 0.19132056832313538 \n",
            "\n",
            "Step 200, Loss target discriminator 0.12324647605419159 \n",
            "\n",
            "Min_loss 0.07188428938388824, min_epoch 7 \n",
            "\n",
            "Starting epoch 9/25, LR = [0.005]\n",
            "Step 210, Loss source 0.00015544891357421875 \n",
            "\n",
            "Step 210, Loss source discriminator 0.06489463150501251 \n",
            "\n",
            "Step 210, Loss target discriminator 0.03866442292928696 \n",
            "\n",
            "Step 220, Loss source 0.018631942570209503 \n",
            "\n",
            "Step 220, Loss source discriminator 0.029636669903993607 \n",
            "\n",
            "Step 220, Loss target discriminator 0.0757991150021553 \n",
            "\n",
            "Step 230, Loss source 0.06618249416351318 \n",
            "\n",
            "Step 230, Loss source discriminator 0.034056663513183594 \n",
            "\n",
            "Step 230, Loss target discriminator 0.14085906744003296 \n",
            "\n",
            "Min_loss 0.07188428938388824, min_epoch 7 \n",
            "\n",
            "Starting epoch 10/25, LR = [0.005]\n",
            "Step 240, Loss source 0.007073543965816498 \n",
            "\n",
            "Step 240, Loss source discriminator 0.07993205636739731 \n",
            "\n",
            "Step 240, Loss target discriminator 0.010819674469530582 \n",
            "\n",
            "Step 250, Loss source 0.0024330317974090576 \n",
            "\n",
            "Step 250, Loss source discriminator 0.06670738756656647 \n",
            "\n",
            "Step 250, Loss target discriminator 0.03094400092959404 \n",
            "\n",
            "Min_loss 0.07188428938388824, min_epoch 7 \n",
            "\n",
            "Starting epoch 11/25, LR = [0.005]\n",
            "Step 260, Loss source 0.008400581777095795 \n",
            "\n",
            "Step 260, Loss source discriminator 0.2465057522058487 \n",
            "\n",
            "Step 260, Loss target discriminator 0.020874539390206337 \n",
            "\n",
            "Step 270, Loss source 0.0042763277888298035 \n",
            "\n",
            "Step 270, Loss source discriminator 0.02264505997300148 \n",
            "\n",
            "Step 270, Loss target discriminator 0.04599160701036453 \n",
            "\n",
            "Step 280, Loss source 0.04723433405160904 \n",
            "\n",
            "Step 280, Loss source discriminator 0.12052615731954575 \n",
            "\n",
            "Step 280, Loss target discriminator 0.147441565990448 \n",
            "\n",
            "Min_loss 0.07188428938388824, min_epoch 7 \n",
            "\n",
            "Starting epoch 12/25, LR = [0.005]\n",
            "Step 290, Loss source 0.04540598392486572 \n",
            "\n",
            "Step 290, Loss source discriminator 0.08439642190933228 \n",
            "\n",
            "Step 290, Loss target discriminator 0.12910829484462738 \n",
            "\n",
            "Step 300, Loss source 0.0012383535504341125 \n",
            "\n",
            "Step 300, Loss source discriminator 0.052727580070495605 \n",
            "\n",
            "Step 300, Loss target discriminator 0.07709456980228424 \n",
            "\n",
            "Step 310, Loss source 0.01018812507390976 \n",
            "\n",
            "Step 310, Loss source discriminator 0.07241211086511612 \n",
            "\n",
            "Step 310, Loss target discriminator 0.14862222969532013 \n",
            "\n",
            "Min_loss 0.04664798825979233, min_epoch 11 \n",
            "\n",
            "Starting epoch 13/25, LR = [0.005]\n",
            "Step 320, Loss source 0.010220393538475037 \n",
            "\n",
            "Step 320, Loss source discriminator 0.05975593999028206 \n",
            "\n",
            "Step 320, Loss target discriminator 0.04666414484381676 \n",
            "\n",
            "Step 330, Loss source 0.007190100848674774 \n",
            "\n",
            "Step 330, Loss source discriminator 0.01947159320116043 \n",
            "\n",
            "Step 330, Loss target discriminator 0.11860261112451553 \n",
            "\n",
            "Min_loss 0.04664798825979233, min_epoch 11 \n",
            "\n",
            "Starting epoch 14/25, LR = [0.005]\n",
            "Step 340, Loss source 0.0004721507430076599 \n",
            "\n",
            "Step 340, Loss source discriminator 0.04046187549829483 \n",
            "\n",
            "Step 340, Loss target discriminator 0.013622527942061424 \n",
            "\n",
            "Step 350, Loss source 0.0021832361817359924 \n",
            "\n",
            "Step 350, Loss source discriminator 0.01110161654651165 \n",
            "\n",
            "Step 350, Loss target discriminator 0.15584619343280792 \n",
            "\n",
            "Step 360, Loss source 0.0006388723850250244 \n",
            "\n",
            "Step 360, Loss source discriminator 0.038704369217157364 \n",
            "\n",
            "Step 360, Loss target discriminator 0.09036114066839218 \n",
            "\n",
            "Min_loss 0.03497110307216644, min_epoch 13 \n",
            "\n",
            "Starting epoch 15/25, LR = [0.005]\n",
            "Step 370, Loss source 0.006477326154708862 \n",
            "\n",
            "Step 370, Loss source discriminator 0.01897032931447029 \n",
            "\n",
            "Step 370, Loss target discriminator 0.02006271854043007 \n",
            "\n",
            "Step 380, Loss source 0.01369088888168335 \n",
            "\n",
            "Step 380, Loss source discriminator 0.032531581819057465 \n",
            "\n",
            "Step 380, Loss target discriminator 0.08653175830841064 \n",
            "\n",
            "Min_loss 0.03497110307216644, min_epoch 13 \n",
            "\n",
            "Starting epoch 16/25, LR = [5e-05]\n",
            "Step 390, Loss source 0.0018633008003234863 \n",
            "\n",
            "Step 390, Loss source discriminator 0.18115922808647156 \n",
            "\n",
            "Step 390, Loss target discriminator 0.04443879425525665 \n",
            "\n",
            "Step 400, Loss source 0.0004770830273628235 \n",
            "\n",
            "Step 400, Loss source discriminator 0.07656392455101013 \n",
            "\n",
            "Step 400, Loss target discriminator 0.008065836504101753 \n",
            "\n",
            "Step 410, Loss source 0.0009158402681350708 \n",
            "\n",
            "Step 410, Loss source discriminator 0.07727207243442535 \n",
            "\n",
            "Step 410, Loss target discriminator 0.009085326455533504 \n",
            "\n",
            "Min_loss 0.03196593374013901, min_epoch 15 \n",
            "\n",
            "Starting epoch 17/25, LR = [0.0005]\n",
            "Step 420, Loss source 0.00016501545906066895 \n",
            "\n",
            "Step 420, Loss source discriminator 0.02667122893035412 \n",
            "\n",
            "Step 420, Loss target discriminator 0.031799476593732834 \n",
            "\n",
            "Step 430, Loss source 0.0019164681434631348 \n",
            "\n",
            "Step 430, Loss source discriminator 0.048971809446811676 \n",
            "\n",
            "Step 430, Loss target discriminator 0.005136232823133469 \n",
            "\n",
            "Step 440, Loss source 0.0003026053309440613 \n",
            "\n",
            "Step 440, Loss source discriminator 0.05821489915251732 \n",
            "\n",
            "Step 440, Loss target discriminator 0.03481392189860344 \n",
            "\n",
            "Min_loss 0.02623523771762848, min_epoch 16 \n",
            "\n",
            "Starting epoch 18/25, LR = [0.0005]\n",
            "Step 450, Loss source 0.0017590373754501343 \n",
            "\n",
            "Step 450, Loss source discriminator 0.030253350734710693 \n",
            "\n",
            "Step 450, Loss target discriminator 0.0733773410320282 \n",
            "\n",
            "Step 460, Loss source 0.00044245272874832153 \n",
            "\n",
            "Step 460, Loss source discriminator 0.022328227758407593 \n",
            "\n",
            "Step 460, Loss target discriminator 0.01605919748544693 \n",
            "\n",
            "Min_loss 0.01811312325298786, min_epoch 17 \n",
            "\n",
            "Starting epoch 19/25, LR = [0.0005]\n",
            "Step 470, Loss source 0.0007886141538619995 \n",
            "\n",
            "Step 470, Loss source discriminator 0.012725736945867538 \n",
            "\n",
            "Step 470, Loss target discriminator 0.030429651960730553 \n",
            "\n",
            "Step 480, Loss source 0.004560582339763641 \n",
            "\n",
            "Step 480, Loss source discriminator 0.053833238780498505 \n",
            "\n",
            "Step 480, Loss target discriminator 0.02951166033744812 \n",
            "\n",
            "Step 490, Loss source 0.006081476807594299 \n",
            "\n",
            "Step 490, Loss source discriminator 0.003399362787604332 \n",
            "\n",
            "Step 490, Loss target discriminator 0.04911133646965027 \n",
            "\n",
            "Min_loss 0.016523411497473717, min_epoch 18 \n",
            "\n",
            "Starting epoch 20/25, LR = [0.0005]\n",
            "Step 500, Loss source 0.0002725869417190552 \n",
            "\n",
            "Step 500, Loss source discriminator 0.010235277004539967 \n",
            "\n",
            "Step 500, Loss target discriminator 0.004647872410714626 \n",
            "\n",
            "Step 510, Loss source 0.0003156214952468872 \n",
            "\n",
            "Step 510, Loss source discriminator 0.019947027787566185 \n",
            "\n",
            "Step 510, Loss target discriminator 0.016715919598937035 \n",
            "\n",
            "Min_loss 0.014980853535234928, min_epoch 19 \n",
            "\n",
            "Starting epoch 21/25, LR = [0.0005]\n",
            "Step 520, Loss source 0.00032617151737213135 \n",
            "\n",
            "Step 520, Loss source discriminator 0.0841953456401825 \n",
            "\n",
            "Step 520, Loss target discriminator 0.018202032893896103 \n",
            "\n",
            "Step 530, Loss source 0.0006766617298126221 \n",
            "\n",
            "Step 530, Loss source discriminator 0.025753717869520187 \n",
            "\n",
            "Step 530, Loss target discriminator 0.010145838372409344 \n",
            "\n",
            "Step 540, Loss source 0.0011346563696861267 \n",
            "\n",
            "Step 540, Loss source discriminator 0.02756379172205925 \n",
            "\n",
            "Step 540, Loss target discriminator 0.020454376935958862 \n",
            "\n",
            "Min_loss 0.014980853535234928, min_epoch 19 \n",
            "\n",
            "Starting epoch 22/25, LR = [0.0005]\n",
            "Step 550, Loss source 0.000835418701171875 \n",
            "\n",
            "Step 550, Loss source discriminator 0.023013904690742493 \n",
            "\n",
            "Step 550, Loss target discriminator 0.011355392634868622 \n",
            "\n",
            "Step 560, Loss source 0.0009558647871017456 \n",
            "\n",
            "Step 560, Loss source discriminator 0.013922600075602531 \n",
            "\n",
            "Step 560, Loss target discriminator 0.018774960190057755 \n",
            "\n",
            "Step 570, Loss source 0.00041732192039489746 \n",
            "\n",
            "Step 570, Loss source discriminator 0.02027171477675438 \n",
            "\n",
            "Step 570, Loss target discriminator 0.0960824266076088 \n",
            "\n",
            "Min_loss 0.014980853535234928, min_epoch 19 \n",
            "\n",
            "Starting epoch 23/25, LR = [0.0005]\n",
            "Step 580, Loss source 0.00042378902435302734 \n",
            "\n",
            "Step 580, Loss source discriminator 0.008455461822450161 \n",
            "\n",
            "Step 580, Loss target discriminator 0.020813114941120148 \n",
            "\n",
            "Step 590, Loss source 0.0006949156522750854 \n",
            "\n",
            "Step 590, Loss source discriminator 0.01424490474164486 \n",
            "\n",
            "Step 590, Loss target discriminator 0.022169312462210655 \n",
            "\n",
            "Min_loss 0.014980853535234928, min_epoch 19 \n",
            "\n",
            "Starting epoch 24/25, LR = [0.0005]\n",
            "Step 600, Loss source 0.001894824206829071 \n",
            "\n",
            "Step 600, Loss source discriminator 0.03635500371456146 \n",
            "\n",
            "Step 600, Loss target discriminator 0.07440692186355591 \n",
            "\n",
            "Step 610, Loss source 0.00044414401054382324 \n",
            "\n",
            "Step 610, Loss source discriminator 0.016083771362900734 \n",
            "\n",
            "Step 610, Loss target discriminator 0.010387072339653969 \n",
            "\n",
            "Step 620, Loss source 0.00026789307594299316 \n",
            "\n",
            "Step 620, Loss source discriminator 0.025386031717061996 \n",
            "\n",
            "Step 620, Loss target discriminator 0.07300509512424469 \n",
            "\n",
            "Min_loss 0.014980853535234928, min_epoch 19 \n",
            "\n",
            "Starting epoch 25/25, LR = [0.0005]\n",
            "Step 630, Loss source 0.0001351684331893921 \n",
            "\n",
            "Step 630, Loss source discriminator 0.010504677891731262 \n",
            "\n",
            "Step 630, Loss target discriminator 0.01889899931848049 \n",
            "\n",
            "Step 640, Loss source 4.0978193283081055e-05 \n",
            "\n",
            "Step 640, Loss source discriminator 0.026233384385704994 \n",
            "\n",
            "Step 640, Loss target discriminator 0.010025128722190857 \n",
            "\n",
            "Min_loss 0.014980853535234928, min_epoch 19 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsHFI-GAJd69"
      },
      "source": [
        "**Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EO3HV5pqJg1o",
        "outputId": "36ebec18-8eb9-49d7-90d7-58dd7623dfa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "\n",
        "net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "net.train(False) # Set Network to evaluation mode\n",
        "\n",
        "running_corrects = 0\n",
        "for images, labels in tqdm(test_dataloader):\n",
        "  images = images.to(DEVICE)\n",
        "  labels = labels.to(DEVICE)\n",
        "\n",
        "  # Forward Pass\n",
        "  outputs = net(images)\n",
        "\n",
        "  # Get predictions\n",
        "  _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "  # Update Corrects\n",
        "  running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "# Calculate Accuracy\n",
        "accuracy = running_corrects / float(len(test_dataset))\n",
        "\n",
        "print('Test Accuracy: {}'.format(accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|| 32/32 [00:07<00:00,  4.14it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.50048828125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}
